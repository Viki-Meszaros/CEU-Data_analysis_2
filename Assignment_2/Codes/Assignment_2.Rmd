---
title: "Predictive analysis on Titanic data"
author: "Viktoria Meszaros"
date: 2021
output: 
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE, warning=F, message= F}
library(tidyverse)
library(ggcorrplot)
library(ggthemes)
library(stargazer)
library(data.table)
require(scales)
library(lspline)
library(estimatr)
library(segmented) 
library(mfx)
library(margins)
library(pscl)
library(modelsummary)
library(texreg)
library(xtable)
library(moments)
library(car)
library(tinytex)

installed.packages("late")

tinytex::install_tinytex()

df <- read.csv("https://raw.githubusercontent.com/Viki-Meszaros/CEU-Data_analysis_2/main/Assignment_2/Data/Clean/Titanic_clean.csv")

df2  <- read.csv("https://raw.githubusercontent.com/Viki-Meszaros/CEU-Data_analysis_2/main/Assignment_2/Data/Raw/Titanic_raw.csv", na.strings = c("", "NA"))
df$X <- NULL

```


# Topic
I chose a really interesting topic for my analysis. It was the tragedy of the luxury steamship called Titanic. As most of you already know, Titanic was one of the first ocean liners and the largest ship in the world at that time. It was traveling from Southampton to New York when an unexpected accident happened. It bumped into an iceberg and sank in the early hours of April 15, 1912. This was one of the deadliest maritime catastrophes of all time leading to the death of more than 1500 people (out of the 2224 on board). 

# Aim of the research
In this project my aim will be to build a predictive model that helps to find out what sort of people were more likely to survive the sinking of the Titanic than others. If there are some factors that increased the probability for someone to survive. 


## Data collection
The data I used for my research I found on the website [Kaggle.com](https://www.kaggle.com/c/titanic/data?select=test.csv) . It contains 819 observations (and 418 in a test file). This is because this data was uploaded as part of a machine learning competition where it is needed to have a train and than a test set. I could not join these two samples, as for the test data the binary values of if the passenger survived or not were missing, so I would not be able to build a model on that. 
On the Titanic there were 2435 people when it sunk, out of which 1320 were passengers and the remaining 892 crew members. In our data set we have data about 819 passengers which is more than 60% of the total population. We know that our sample was created in a totally random manner so we can assume that it is representative. 
There were some missing values in my data for some observation representing 8% of our data. Most of the missing values are in Cabin numbers and Age. 

```{r, echo= F, warning= F, message= F}
nas <- NULL
for (i in 1:ncol(df2)) {
  nas  <- c(nas, sum(is.na(df2[ , i])))
}

num_NAs <- as_tibble(cbind(names(df2), nas))
colnames(num_NAs) <- c("Variable", "Number_of_NAs")

knitr::kable(num_NAs %>%  arrange( desc(Number_of_NAs)) %>% head(3) )

```

As Age is an important variable for my analysis I did some imputation to replace NA values. As the distribution of age was skewed to the right instead of mean value I used the median to decrease the bias. I calculated the different median age values for males and females in different social groups as I found out from the data that women were younger on Titanic and people in the 1st class were older on average compared to the ones in lower social classes. The following table shows the different values I imputed for the different groups.

```{r, echo= F, warning= F, message= F, out.width= "50%"}

age <- df2 %>% 
  group_by(Sex, Pclass) %>%
  summarise("average age" = round(median(Age, na.rm = T)))

knitr::kable(age)
```

Although this imputation helps to get rid of missing values it also causes bias in my estimation as these values are just my assumptions instead of actual values. According to data quality there is one more issue to mention. There is a variable called Pclass which shows the ticket class a given passenger had. From this we will assume that these passengers belong to that socio-economic layer. This variable is interesting as out initial assumption is that people who belong to the 1st class will have a higher probability of survival as they were more important is the eyes of crew during the evacuation process. This assumption may not be true if for example a really wealthy person from social class 1 bought a 2nd or 3rd class ticket as he/she does not care about luxury during his/her travel. 
We should also keep in mind the this tragedy happened in 1912 when administration was not even close to perfect and due to this our data may contain some measurement errors due to personal mistakes or lost documents.

## Data cleaning

During the data cleaning process I dealt with missing values. I excluded Cabin variable as 77% of the values were missing. For age as I already outlined I made imputation as I considered it an important variable for my prediction. Last but not least for Embarked there were 2 missing values. I decided to do some extra research as I knew the name of the passengers. I found out that both of them embarked in Southhampton, so I filled in this values which solved the problem. After dealing with missing values I also looked at extreme values. I only had two continuous variables Age and Fare, so I only had to look for extremes here. For Age the distribution looked close to normal with no unexpected extreme values. For Fare the distribution was rather skewed to the right, due to some extreme values. I had a closer look on these, and realized these were not errors but the most expensive  tickets so I decided not to exclude any values.
I also had to do some transformations on categorical variables to make them appropriate for the modeling. I had some data about how many parent, siblings, spouses or children a passenger had on board with them. From this I decided to create a binary variable Travel_alone which was was if the person was alone on board and 0 if he/she had some family members with them. I though this variable will be helpful as it was said that during the rescue families, especially women or men with children had higher priority to get on the boats. For social class and embark location I created dummy variables with n-1 categories always leaving out one category as a base. For more information about my data cleaning process please check my [data_cleaning.R](https://github.com/Viki-Meszaros/CEU-Data_analysis_2/blob/main/Assignment_2/Codes/1_data_cleaning.R) available in my Github repository. 

**For my model I will use:**

- Survived (0 if no, 1 if yes) as the dependent(*y*) variable
- Sex, Age, Social Class, Embarked, Ticket price and if someone traveled alone or with family are going to be the right hand side(*x*) variables for my model

# Patterns of association

After looking at all the patterns of association between my outcome variable and all the explanatory variables I could come up with some interesting assumptions. (For all the graphs of the patterns of association please look at the Appendix section)

- Women had more chance to survived then men.
- People in higher social class had higher probability of survival. Someone in socio-economic class 3 had the worst chance.
- Those who embarked in Southhampton died with higher probability and those who got on board in Cherboug had the highest chance of surviving.
- Someone who traveled with his/her family also were in a better situation as for them survival was more likely compared to someone traveling alone.
- The probability to survive decreased with age for people below 25 and above 32, but increased between these two
- As the price people paid for the travel increased, so did the probability of survival

From the graphs below you can see the patterns of association between the two continuous variables and the dependent variable. Based on this I decided to use a piecewise linear spline for Age and the log transformation for the Fare variable later on.

```{r, echo= F, message= F, warning= F, out.width= "33%"}
#AGE
ggplot( df , aes( x = Age , y = Survived ) )+
  stat_summary_bin( fun = 'mean' , binwidth = 7, 
                    geom = 'point',  size = 2 ) +
  labs( x = 'Age' , y = 'Survived' )+
  theme(legend.position = "none")+
  geom_smooth(method="loess",formula = y~x, color = "cyan4") +
  labs(title = "Pattern od association between age and survival (bins of 7 age)",x = "Age",y = "Survived / Predicted probability of ")+
  theme_calc()

#FARE
df %>% 
  ggplot(aes(x = Fare, y = Survived)) +
  stat_summary_bin( fun = 'mean' , binwidth = 10, 
                    geom = 'point',  size = 2 ) +
  geom_smooth(method = "loess", color = "cyan4") +
  labs(title = "Pattern od association between fare and survival (bins of 10 USD)", x = "Fare",y = "Survived / Predicted probability of ")+
  theme_calc()

# with binned values
df %>% 
  ggplot(aes(x = Fare, y = Survived)) +
  stat_summary_bin( fun = 'mean' , binwidth = 0.5, 
                    geom = 'point',  size = 2 ) +
  scale_x_continuous( trans = "log") +
  geom_smooth(method = "loess", color = "cyan4") +
  labs(title = "Pattern od association between ln(Far) and survival (bins of 0.5 ln(fare))", x = "ln(Fare)",y = "Survived / Predicted probability of ")+
  theme_calc()
```

## Correlation between variables

Before building my model I checked the correlation between my variables. From this graph you can see that there is only one variable pair whose correlation is high and that is Pclass and Pclass_1. This is what we expect as they measure the same thing. We have moderate correlation between Pclass and Fare as Pclass measures the class of the ticket its price should increase if it belongs to a higher class.Also there is moderate correlation between Female and Survived meaning that the sex of the passenger strongly correlated with the probability of survival. For our model the only correlation that matters is between the fare and Pclass. Due to this I decided not to use fare in my model only social class. This also made my final model simpler, with keeping its power.

```{r, echo= F, warning= F, message= F, out.width= "50%", fig.align= 'center'}
numeric_df <- keep( df , is.numeric )
corr <- round(cor( 
  keep(df, is.numeric)), 3)

  ggcorrplot(corr)

```

# Model formula
As I did prediction I decided to split my data to two sets. First to a training set and then a test set to do robustness check in the end. I put randomly 80% of my observations to train and left the remaining 20% in a test set. 

I ran my predictive models on the train set. I tried to predict if someone survived the sinking of Titanic or not when I know their sex, if they traveled alone or with their families, their social class (1st, 2nd or 3rd), where they embarked (Queenstown = Q, Cherbourg = C or Southhampton = left-out category) and their age.
```{r, echo= F, warning= F, message= F}
set.seed(123)
dt <- sort(sample(seq_len(nrow(df)), nrow(df)*.8))

test <- df[-dt,]
train <- df[dt, ]

```

$$P(Survived)^P = Female + Travelalone + Pclass_1 + Pclass_2 + Q + C + lspline(Age, c(27, 32))$$
For this model formula I ran a liner probability model, a logit and a probit model to find the one that gives the best results.
This table shows  the coefficients for the **LPM** model (*Model 1*) and the marginal effects for the **logit** (*Model 2*) and **probit** (*Model 3*) models.

```{r, echo= F, warning= F, message= F}
model_formula <- formula( Survived ~ Female + Pclass_1 + Pclass_2 + lspline(Age, c(27, 32)) 
                           + Travel_alone + Q + C )
#### LPM
lpm <-lm( model_formula , data=train)

train$pred_lpm <- predict(lpm)


train <- train %>% 
  mutate( pred_lpm_100 = ntile(pred_lpm, 100) )

#### LOGIT
logit <- glm( model_formula , data=train, family=binomial(link="logit") )

train$pred_logit <- predict.glm(logit, type="response")

logit_marg <- logitmfx( model_formula, data=train, atmean=FALSE, robust = T)


#### PROBIT
probit <- glm( model_formula , data = train , family=binomial(link="probit") )

train$pred_probit<- predict.glm( probit , type = "response" )

probit_marg <- probitmfx(  model_formula, data=train, atmean=FALSE, robust = T)

cm <- c('(Intercept)' = 'Constant')
pmodels <- list(lpm,  logit_marg,  probit_marg)
msummary( pmodels ,
          fmt="%.3f",
          gof_omit = 'train|Deviance|Log.Lik.|F|R2 Adj.|AIC|BIC|R2|PseudoR2',
          stars=c('*' = .05, '**' = .01),
          coef_rename = cm,
          coef_omit = 'as.factor(country)*',
        
)



```

## Model interpretation

We can see that the values are really similar for all of the variables meaning that the functional form of my explanatory variables seems to be good. 

- With 99% confidence we can assume that the probability for a women to survive is 50% higher than for a men
- With 99% confidence a person belonging to the 1st social class has 34% higher probability to survive compared to someone from the 3rd social class, while someone in the 2nd class has 18% higher chance
- We can be 99% confident that under the age of 27 and over the age of 32 the probability of survival is expected to be 1% lower for someone one year older 
- Also there is 99% confidence that those who embarked in Cherbourg has 11,5% more probability to survive compared to those who embarked in Southhampton. And with 95% confidence we can also state that people embarked in Queenstown had 11-12% more probability to survive than who got on board in Southhampton
- We cannot be confident that traveling alone had any effect on the probability of survival

## Final model
When deciding between LPM, logit and probit models I looked at the goodness of fit with BIC, checked model fit with brier and also made some model diagnostics like bias and calibration curves.
We can see that the BIC scores are similar but it is the smallest for the logit model. Considering the bias values LPM and logit seems to be unbiased (0.000 bias score values) on the other hand for probit we get 0.003 which is low as well but compared to the others really high.  The Brier score is also similar, but it is the lowest for the logit model. Anf last but not least the prediction accuracy is 80% fro the LPM while 81% for logit and probit models.


```{r, echo= F, warning= F, message= F, out.width= "50%", fig.align='center' }
# Got this part from Brúnó many thanks for his help!

stats <- transpose(glance(lpm))
names(stats) <- "LPM"
StatNames <- names(glance(lpm))
BaseStats <- cbind("Stats" = StatNames, round(stats,2))

stats <- transpose(glance(logit))
names(stats) <- "Logit"
StatNames <- names(glance(logit))
LogStats <- cbind("Stats" = StatNames, round(stats,2))

stats <- transpose(glance(probit))
names(stats) <- "Probit"
StatNames <- names(glance(probit))
ProbStats <- cbind("Stats" = StatNames, round(stats,2))

goodness <- BaseStats[7:9,] %>% left_join(ProbStats[3:5,], by = "Stats") %>% 
  left_join(LogStats[3:5,], by = "Stats")

rm(BaseStats, LogStats, ProbStats)

# accuracy
accuracy <- as.data.frame(lapply(c("pred_lpm","pred_logit","pred_probit"),function(x) {
  tl <- list()
  tl[[x]] <- round(sum(round(train[x],0) == train$Survived)/length(train$Survived),3)*100
  return(tl)
}))

accuracy$Stats <- "Prediction Accuracy (%)"

# Brier
briers <- as.data.frame(lapply(c("pred_lpm","pred_logit","pred_probit"),function(x) {
  tl <- list()
  tl[[x]] <- round(sum(  (train[c(x)]-train[c("Survived")])^2)/count(train[c("Survived")]),3)
  names(tl[[x]]) <- x
  return(tl)
}))

briers$Stats <- "Brier-Score"

# Bias
bias <- as.data.frame(lapply(c("pred_lpm","pred_logit","pred_probit"), function(x) {
  tl <- list()
  Pred <- sum(train[c(x)]) / count(train[c(x)])
  Act <- sum(train[c("Survived")]) / count(train[c("Survived")])
  
  tl[[x]] <- round(Pred - Act,3)
  names(tl[[x]]) <- x
  return(tl)
}))

bias$Stats <- "Bias"

# merge to final table
goodness <- rbind(goodness
                    ,bias %>% dplyr::select(Stats, pred_lpm, pred_probit, pred_logit) %>% 
                      rename("LPM" = pred_lpm, "Logit" = pred_logit, "Probit" = pred_probit)
                    ,briers %>% dplyr::select(Stats, pred_lpm, pred_probit, pred_logit) %>% 
                      rename("LPM" = pred_lpm, "Logit" = pred_logit, "Probit" = pred_probit)
                    ,accuracy %>% dplyr::select(Stats, pred_lpm, pred_probit, pred_logit) %>% 
                      rename("LPM" = pred_lpm, "Logit" = pred_logit, "Probit" = pred_probit))

rownames(goodness) <- NULL

knitr::kable(goodness)
```

From the graphs below we can also conclude that all of the models are quite well calibrated, as the predicted probabilities tend to move along the 45° line on the y ~ y^ curve. 

```{r, echo= F, warning= F, message= F, out.width= "33%"}
#### LPM
actual_vs_predicted <- train %>%
  ungroup() %>% 
  dplyr::select(actual = Survived, 
                predicted = pred_lpm) 
num_groups <- 10

calibration_d <- actual_vs_predicted %>%
  mutate(predicted_score_group = dplyr::ntile(predicted, num_groups))%>%
  group_by(predicted_score_group) %>%
  dplyr::summarise(mean_actual = mean(actual), 
                   mean_predicted = mean(predicted), 
                   num_obs = n())

ggplot( calibration_d,aes(x = mean_actual, y = mean_predicted)) +
  geom_point( color='orchid', size=1.5) +
  geom_line(  color='orchid', size=1) +
  geom_abline( intercept = 0, slope = 1, color='cyan4', size = 1) +
  labs(title = "LPM model", x = "Actual event probability", y = "Predicted event probability") +
  scale_x_continuous(expand = c(0.01,0.01), limits = c(0,1), breaks = seq(0,1,0.1)) +
  scale_y_continuous(expand = c(0.01,0.01), limits = c(0,1), breaks = seq(0,1,0.1)) +
  theme_calc()

#### LOGIT
actual_vs_predicted <- train %>%
  ungroup() %>% 
  dplyr::select(actual = Survived, 
                predicted = pred_logit) 
num_groups <- 10

calibration_d <- actual_vs_predicted %>%
  mutate(predicted_score_group = dplyr::ntile(predicted, num_groups))%>%
  group_by(predicted_score_group) %>%
  dplyr::summarise(mean_actual = mean(actual), 
                   mean_predicted = mean(predicted), 
                   num_obs = n())

ggplot( calibration_d,aes(x = mean_actual, y = mean_predicted)) +
  geom_point( color='orchid', size=1.5) +
  geom_line(  color='orchid', size=1) +
  geom_abline( intercept = 0, slope = 1, color='cyan4', size = 1) +
  labs(title = "Logit model", x = "Actual event probability", y = "Predicted event probability") +
  scale_x_continuous(expand = c(0.01,0.01), limits = c(0,1), breaks = seq(0,1,0.1)) +
  scale_y_continuous(expand = c(0.01,0.01), limits = c(0,1), breaks = seq(0,1,0.1)) +
  theme_calc()

#### PROBIT
actual_vs_predicted <- train %>%
  ungroup() %>% 
  dplyr::select(actual = Survived, 
                predicted = pred_probit) 
num_groups <- 10

calibration_d <- actual_vs_predicted %>%
  mutate(predicted_score_group = dplyr::ntile(predicted, num_groups))%>%
  group_by(predicted_score_group) %>%
  dplyr::summarise(mean_actual = mean(actual), 
                   mean_predicted = mean(predicted), 
                   num_obs = n())

ggplot( calibration_d,aes(x = mean_actual, y = mean_predicted)) +
  geom_point( color='orchid', size=1.5) +
  geom_line(  color='orchid', size=1) +
  geom_abline( intercept = 0, slope = 1, color='cyan4', size = 1) +
  labs(title = "Proit model", x = "Actual event probability", y = "Predicted event probability") +
  scale_x_continuous(expand = c(0.01,0.01), limits = c(0,1), breaks = seq(0,1,0.1)) +
  scale_y_continuous(expand = c(0.01,0.01), limits = c(0,1), breaks = seq(0,1,0.1)) +
  theme_calc()
```


*Due to these reasons and values I picked the logit model as my final choice as that is the best according to most of the metrics.*


## Analyse predicted probabilities
#### Highest probability of survival
From the model we could find out that the lukiest ones who had the highest probabbility of survival were on average 24 years old women belonging the the 1st socio-economic class, who embarked in Cherbourg and travelled alone travelled alone.
```{r, echo= F, warning= F, message= F, out.width= "50%"}
train <- train %>% 
  mutate( pred_logit_100 = ntile(pred_logit, 100) )

source("https://raw.githubusercontent.com/Viki-Meszaros/CEU-Data_analysis_2/main/Assignment_2/Codes/sum_stat.R")

top <- sum_stat( subset( train , pred_logit_100==100 ) , 
          c('Female', 'Travel_alone', "Pclass_1", "Pclass_2", "C", "Q", "Age"),
          c('mean','median','sd'),
          num_obs = F )


knitr::kable(top)

```

#### Lowest probability of survival
On the other hand the worst chances belonged to men with an age around 54 coming from the lowest socio-economic class, embarking in Southhampton who travelleda alone. 
```{r, echo= F, warning= F, message= F, out.width= "50%"}
bottom <- sum_stat( subset( train , pred_logit_100==1 ) , 
          c('Female', 'Travel_alone', "Pclass_1", "Pclass_2", "C", "Q", "Age"),
          c('mean','median','sd'),
          num_obs = F )

knitr::kable(bottom)

```


## Robustness check
From the comparison of the coefficient and the marginal effect of out models we can say that the functional form I chose for the explanatory variables seems to be good. In my [analysis.R](https://github.com/Viki-Meszaros/CEU-Data_analysis_2/blob/main/Assignment_2/Codes/2_analysis.R) file you can also check that I built my final model formula through several steps including my variables step by step. The coefficents/marginal effects are close to constant throughout my models showing that they actually has an effect of the specific size on the outcome variable. 
For robustness check I also ran the final model on my test data set. From the confusion table we can see that the model has similar accuracy as for the training data set with 77% of prediction accuracy. I am satisfied with this result, I think this shows that my model is quite robust.

```{r, echo= F, warning= F, message= F}
test$pred <- predict.glm(logit, test, type = "response")

threshold <- 0.5

# Decide for each observations and each prediction, if larger than the threshold value!
for (i in 1:nrow(test)) {
   if (test$pred[i] > threshold) {test$pred[i]=1}
    else {test$pred[i]=0}
  }

conf_table <- prop.table(table(test$pred, test$Survived))

knitr::kable(conf_table)

```

## External validity

I think the external validity of my prediction is really low. It could be used to predict the survival of passengers on Titanic, but not on other boats. Here most of the findings that they saved women and children and people who belonged to higher social classes may not be relevant in the world we have today. I would also say that there are better explanatory variables to predict survival in a catastrophe like this for example distance to rescue ships or health of a passenger. The aim of this analysis was to find out if I can build a model that predicts death in this very situation rather than to create a tool for other, later use. 

# Conclusions
The most important findings of my analysis was that on Titanic during its sinking young females with an average age of 24 from the highest social class had the highest probability of survival. This is most probably because during the evacuation they prioritized women over men and higher social classes had also priority compared to lower ones. What surprised me is that those who embarked in Cherbourg had also significantly higher probability of survival. This may due to that the cabins were assigned by embarking and those who embarked at the first stop Southhampton got cabins on lower levels that those who got on the boat in Cherbourg. Another reason can be that more 1st class people embarked there. Overall I think these are pretty interesting findings andare close to the reality. I also managed to build a model on a random traning set (with 81% predicting accuracy) with which I could predict 77% of my test sample corretly. This shows to me that my model is highly valid for other sets of passengers on Titanic, but probably not really valid for other cases.


# APPENDIX

## I. Data descriptives

### Continuous variables

```{r, echo= F, message= F, warning= F, out.width= "40%"}

# AGE
    ggplot(df2, aes(x = Age)) +
    geom_histogram(aes(y = ..density..), fill = "cyan4", color = "white", alpha = 0.6, binwidth = 5) +
    geom_density(aes( y = ..density.. ), color = "cyan4", bw = 5, size = 1) +
    theme_calc() +
    labs(title = "Density plot for Age", x = "Age", y = "")

# FARE  
    ggplot(df2, aes(x = Fare)) +
    geom_histogram(aes(y = ..density..), fill = "cyan4", color = "white", alpha = 0.6, binwidth = 15) +
    geom_density(aes( y = ..density.. ), color = "cyan4", bw = 15, size = 1) +
    theme_calc() +
    labs(title = "Density plot for Fare", x = "Ticket price", y = "")
```

### Summary statistics for continuous variables

```{r, echo= F, warning= F, message= F}
source("https://raw.githubusercontent.com/Viki-Meszaros/CEU-Data_analysis_2/main/Assignment_2/Codes/sum_stat.R")
 
    desc_stat <- sum_stat( df2 , var_names = c('Age', 'Fare'),
                           stats = c('min','1st_qu.', 'median','mean', '3rd_qu','max','sd') )
knitr::kable(desc_stat)
```

### Distribution of Age after imputation
```{r, echo= F, warning= F, message= F, out.width="40%"}
colnames(df2)[6] <- "Age_w_missing"

for (i in 1:nrow(df2)) {
  if (is.na(df2$Age_w_missing[i])) {
    if (df2$Sex[i] == "female") {
      if (df2$Pclass[i] == 1) {
        df2$Age[i] <- 35  
      } else if(df2$Pclass[i] == 2) {
        df2$Age[i] <- 28
      } else { df2$Age[i] <- 22}
    } 
    else {
      if (df2$Pclass[i] == 1) {
        df2$Age[i] <- 41  
      } else if(df2$Pclass[i] == 2) {
        df2$Age[i] <- 31
      } else { df2$Age[i] <- 27}
    }
  }
  else{
    df2$Age[i] <- df2$Age_w_missing[i]
  }
  
}

ggplot(df2, aes(x = Age_w_missing)) +
  geom_histogram(aes(x=Age, y = ..density..), fill = "orchid", color = "white", alpha = 0.7, binwidth = 5 ) +
  geom_histogram(aes(y = ..density..), fill = "cyan4", color = "white", alpha = 0.5, binwidth = 5) +
  geom_density(aes(x = Age, y = ..density.. ), color = "orchid", bw = 5, size = 1) +
  geom_density(aes( y = ..density.. ), color = "cyan4", bw = 5, size = 1) +
  theme_calc() +
  labs(title = "Density plot for Age", x = "", y = "")

```


### Categorical variables 

```{r, echo= F, message= F, warning= F, out.width= "33%"}
# SEX
    ggplot(df2, aes(x = Sex)) +
    geom_bar(fill = "cyan4", alpha = 0.6) +
    theme_calc() +
    labs(title = "Sex of the passengers")

# SURVIVED
    ggplot(df2, aes(x = Survived)) +
    geom_bar(fill = "cyan4", alpha = 0.6) +
    theme_calc() +
    labs(title = "Number of passengers who survived and who died", x="", y="") +
    scale_x_continuous(breaks = c(0,1) ,labels = c("0" = "Died", "1" = "Survived"))
    
# TICKET CLASS
    ggplot(df2, aes(x = Pclass)) +
    geom_bar(fill = "cyan4", alpha = 0.6) +
    theme_calc() +
    labs(title = "Different ticket classes", x="", y="") +
    scale_x_continuous(breaks = c(1, 2, 3) ,labels = c("1" = "1st Class", "2" = "2nd Class", "3" = "3rd Class"))
    
# EMBARKED  
    ggplot(df2, aes(x = Embarked)) +
    geom_bar(fill = "cyan4", alpha = 0.6) +
    theme_calc() +
    labs(title = "Different ticket classes", x="", y="")
    
# NUMBER OF PARENTS OR CHILDREN
    ggplot(df2, aes(x = Parch)) +
    geom_bar(fill = "cyan4", alpha = 0.6) +
    theme_calc() +
    labs(title = "Number of parents and/or children a passenger has", x="", y="")
    
# NUMBER OF SIGLINGS
    ggplot(df2, aes(x = SibSp)) +
    geom_bar(fill = "cyan4", alpha = 0.6) +
    theme_calc() +
    labs(title = "Number of siblings a passenger has", x="", y="")
    

```


## II. Patterns of association 

```{r, echo= F, warning= F, message= F, out.width= "45%"}
#SEX
lpm1_sex <- lm(Survived ~ Female, data=df)
df$pred1 <- predict(lpm1_sex)

df <- df %>%
  group_by(Female, Survived) %>%
  mutate(weight = n())  %>%
  mutate(weight_2=(weight/1000))

ggplot(data = df) +
  geom_line(aes(x = Female, y = pred1),  size = 0.8, color = "cyan3", alpha = 0.6 ) +
  geom_point(aes(x = Female, y = pred1), size = 3, color = "cyan4",  shape = 16) +
  geom_point(aes(x = Female, y = Survived, size=weight_2), color = "purple3", shape = 16, alpha=0.8, show.legend=F, na.rm=TRUE)  +
  labs(x = "Female",y = "Survived / Predicted probability of ")+
  coord_cartesian(xlim = c(0, 1), ylim=c(0,1)) +
  scale_y_continuous(limits = c(0,1), breaks = seq(0,1,0.1))+
  scale_x_continuous(limits = c(0,1), breaks = c(0,1), labels = c("0" = "Male", "1" = "Female"))+
  theme_calc() 

## PCLASS
lpm2_pclass <- lm(Survived ~ Pclass_1 + Pclass_2, data = df)
df$pred2 <- predict(lpm2_pclass)

df <- df %>%
  group_by(Pclass, Survived) %>%
  mutate(weight = n())  %>%
  mutate(weight_2=(weight/1000))

ggplot(data = df) +
  geom_line(aes(x = Pclass, y = pred2),  size = 0.8, color = "cyan3", alpha = 0.6 ) +
  geom_point(aes(x = Pclass, y = pred2), size = 3, color = "cyan4",  shape = 16) +
  geom_point(aes(x = Pclass, y = Survived, size=weight_2), color = "purple3", shape = 16, alpha=0.8, show.legend=F, na.rm=TRUE)  +
  labs(x = "Socio-economic class",y = "Survived / Predicted probability of ")+
  scale_y_continuous(limits = c(0,1), breaks = seq(0,1,0.1))+
  scale_x_continuous(breaks = c(1, 2, 3) , labels = c("1" = "1st Class", "2" = "2nd Class", "3" = "3rd Class" )) +
  theme_calc() 

## EMBARKED
lpm3_embarked <- lm(Survived ~ C + Q, data = df)
df$pred3 <- predict(lpm3_embarked)

df <- df %>%
  group_by(Embarked, Survived) %>%
  mutate(weight = n())  %>%
  mutate(weight_2=(weight/1000))

ggplot(data = df) +
  geom_point(aes(x = Embarked, y = pred3), size = 3, color = "cyan4",  shape = 16) +
  geom_point(aes(x = Embarked, y = Survived, size=weight_2), color = "purple3", shape = 16, alpha=0.8, show.legend=F, na.rm=TRUE)  +
  labs(x = "Embarked in",y = "Survived / Predicted probability of ")+
  scale_y_continuous(limits = c(0,1), breaks = seq(0,1,0.1)) +
  scale_x_discrete(labels = c( "Q" = "Queenstown", "S" = "Southhampton", "C" = "Cherbourg")) +
  theme_calc() 

## TRAVEL ALONE
lpm4_travalone <- lm(Survived ~ Travel_alone, data = df)
df$pred4 <- predict(lpm4_travalone)

df <- df %>%
  group_by(Travel_alone, Survived) %>%
  mutate(weight = n())  %>%
  mutate(weight_2=(weight/1000))

ggplot(data = df) +
  geom_line(aes(x = Travel_alone, y = pred4),  size = 0.8, color = "cyan3", alpha = 0.6 ) +
  geom_point(aes(x = Travel_alone, y = pred4), size = 3, color = "cyan4",  shape = 16) +
  geom_point(aes(x = Travel_alone, y = Survived, size=weight_2), color = "purple3", shape = 16, alpha=0.8, show.legend=F, na.rm=TRUE)  +
  labs(x = "Travel Alone",y = "Survived / Predicted probability of ")+
  scale_y_continuous(limits = c(0,1), breaks = seq(0,1,0.1))+
  scale_x_continuous(limits = c(0,1), breaks = c(0,1), labels = c("0" = "No", "1" = "Yes"))+
  theme_calc() 

```

# III. Modelling

### Comparing predicted probabilities of logit and probit to LPM 
```{r, echo= F, warning= F, message= F, out.width= "60%"}
ggplot(data = train) +
  geom_point(aes(x=pred_lpm, y=pred_probit, color="Probit"), size=1,  shape=16) +
  geom_point(aes(x=pred_lpm, y=pred_logit,  color="Logit"), size=1,  shape=16) +
  geom_line(aes(x=pred_lpm, y=pred_lpm,    color="45 degree line"), size=1) +
  labs(x = "Predicted probability of staying healthy (LPM)", y="Predicted probability")+
  scale_y_continuous(expand = c(0.00,0.0), limits = c(0,1), breaks = seq(0,1,0.1)) +
  scale_x_continuous(expand = c(0.00,0.0), limits = c(0,1), breaks = seq(0,1,0.1)) +
  scale_color_manual(name = "", values=c("forestgreen", "orchid","cyan4"))+
  theme(legend.position=c(0.55,0.08),
        legend.direction = "horizontal",
        legend.text = element_text(size = 4)) +
  theme_calc()
```

### Predicted values by models 
```{r, echo= F, warning= F, message= F, out.width= "33%"}
ggplot( train , aes( x = pred_lpm) ) +
  geom_histogram( fill = 'cyan4' , color = 'grey90', binwidth = 0.08)+
  theme_calc()+
  labs(title = "LPM", x = "Predicted values")

ggplot( train , aes( x = pred_logit) ) +
  geom_histogram( fill = 'cyan4' , color = 'grey90', binwidth = 0.08)+
  theme_calc()+
  labs(title = "Logit", x = "Predicted values")

ggplot( train , aes( x = pred_probit) ) +
  geom_histogram( fill = 'cyan4' , color = 'grey90', binwidth = 0.08)+
  theme_calc()+
  labs(title = "Probit", x = "Predicted values")
```

## Confusion tables
```{r, echo= F, warning= F, message= F}
conf_table <- data.frame(train$pred_lpm, train$pred_logit, train$pred_probit)

# Set the threshold value
threshold <- 0.5

# Decide for each observations and each prediction, if larger than the threshold value!
for (i in 1:nrow(conf_table)) {
  for (j in 1:ncol(conf_table)) {
    if (conf_table[i,j]>threshold) {conf_table[i,j]=1}
    else {conf_table[i,j]=0}
  }
}



# confusion matrix - does it seems similar?
for (j in 1:ncol(conf_table)){
  print(prop.table(table(conf_table[, j], train$Survived)))
}

```


### Summary statistics for all models where Survived == 1
```{r, echo= F, warning= F, message= F, out.width= "50%"}
source("https://raw.githubusercontent.com/Viki-Meszaros/CEU-Data_analysis_2/main/Assignment_2/Codes/sum_stat.R")
 
best <- sum_stat( subset( train , Survived == 1 ) , 
          c( "pred_lpm","pred_logit","pred_probit" ),
          c("mean","median","min","max","sd"),
          num_obs = F )

knitr::kable(best)

```


### Summary statistics for all models where Survived == 0
```{r, echo= F, warning= F, message= F, out.width= "50%"}

worst <- sum_stat( subset( train , Survived == 0 ) , 
                 c( "pred_lpm","pred_logit","pred_probit" ),
                 c("mean","median","min","max","sd"),
                 num_obs = F )

knitr::kable(worst)

```














